# åˆ†åŒºæ•°è°ƒæ•´æ˜¯Spark SQL ä¸­**æœ€é‡è¦**çš„å‚æ•°ï¼Œæ²¡æœ‰ä¹‹ä¸€ã€‚

# *   **é»˜è®¤å€¼**ï¼š200ã€‚
# *   **å«ä¹‰**ï¼šåœ¨è¿›è¡Œ Join æˆ– GroupBy (Shuffle) æ—¶ï¼Œç»“æœä¼šè¢«åˆ†æˆå¤šå°‘ä»½ã€‚

# **åœºæ™¯æ¨¡æ‹Ÿ**ï¼š
# 1.  **å¤ªå° (æ¯”å¦‚ 1)**ï¼šæ‰€æœ‰æ•°æ®æŒ¤åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œï¼Œå¤„ç†ææ…¢ï¼Œå®¹æ˜“ OOMã€‚
# 2.  **å¤ªå¤§ (æ¯”å¦‚ 2000 å¤„ç†å°æ•°æ®)**ï¼šäº§ç”Ÿå¤§é‡å°æ–‡ä»¶ï¼ˆKBçº§åˆ«ï¼‰ï¼ŒSpark èŠ±åœ¨è°ƒåº¦ Task ä¸Šçš„æ—¶é—´æ¯”çœŸæ­£å¹²æ´»çš„æ—¶é—´è¿˜é•¿ã€‚
# ğŸ’¡ è°ƒä¼˜æ³•åˆ™**ï¼š
# *   å®˜æ–¹å»ºè®®ï¼šæ¯ä¸ª Partition å¤„ç†çš„æ•°æ®é‡æœ€å¥½åœ¨ **128MB - 200MB** å·¦å³ã€‚
# *   å¦‚æœä½ çš„ shuffle é˜¶æ®µæ€»æ•°æ®é‡æ˜¯ 10GBã€‚
#     *   10GB = 10240MBã€‚
#     *   åˆé€‚çš„åˆ†åŒºæ•° = 10240 / 128 â‰ˆ 80ã€‚
#     *   æ‰€ä»¥è®¾ `spark.sql.shuffle.partitions = 80` æ¯”é»˜è®¤çš„ 200 å¥½ã€‚

import time
import os,sys
from utils.spark_utils import get_spark_session

def run_job():
    spark = get_spark_session("Partition_Turning","local[2]")

    # é€ 500ä¸‡æ¡æ•°æ®ã€
    df = spark.range(0,5000000).toDF("id")
    # åˆ¶é€ ä¸€ä¸ªéœ€è¦shuffleçš„æ“ä½œ 
    df_grouped = df.groupBy(df.id % 10 ).count()

    # 1 åˆ†åŒºæ•°å–200(å¤ªå¤š)
    spark.conf.set("spark.sql.shuffle,partitions",200)
    start = time.time()
    df_grouped.collect()
    print(f"åˆ†åŒºæ•°200è€—æ—¶: {time.time() - start:.2f}s")

    # 2 åˆ†åŒºæ•°å–10 ï¼ˆåˆé€‚ï¼‰
    spark.conf.set("spark.sql.shuffle.partitions",10)
    start = time.time()
    df_grouped.collect()
    print(f"åˆ†åŒºæ•°10è€—æ—¶: {time.time() - start:.2f}s")

    # 3 åˆ†åŒºæ•°å–1ï¼ˆå¤ªå°‘ï¼‰
    spark.conf.set("spark.sql.shuffle.partitions",1)
    start = time.time()
    df_grouped.collect()
    print(f"åˆ†åŒºæ•°1è€—æ—¶: {time.time() - start:.2f}s")

# if __name__ == "__main__":
#     run_job()